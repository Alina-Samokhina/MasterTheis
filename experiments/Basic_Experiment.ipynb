{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# MsThesis draft"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Progress"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### weeks 1-4 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Started from the end of the second week. Although, all the tools were ready.\n",
    "\n",
    "**What**: neural ODEs, LMU \n",
    "**Why**: \n",
    " - to represent data continuously\n",
    " - to solve problems with:\n",
    "     - irregular sampling\n",
    "     - low sampling rates\n",
    " - to lay groundwork for space-continuous models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**Begin-talk**\n",
    "\n",
    "\n",
    "**The project goal**  \n",
    "The motivation rises from the desire to get algorithms which is able to work with irregularly and rare sampled data from the real-world.  \n",
    "**The main idea**  \n",
    "We have continuous-in-depth LSTM's which allows to represent continuous-in-time signal. \n",
    "**The expected result**  \n",
    "Obtain continuous-time representation of a signals. Check out what applications of the continuity of time there are."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Write:\n",
    "1. Abstract\n",
    "2. Introduction\n",
    "3. Literature\n",
    "\n",
    "For these see [the link](https://www.overleaf.com/read/rjvmxvkmgvyg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "    State your problem, write a description of your basic algorithm, prepare your computational experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "    Basic code, draft report on the basic algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "    Code, visual presentation of results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "    Describe the algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Cложный уровень: выставить гипотезы и условия. Сформулировать и доказать теорему."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "    Make the error and quality analysis. Finalise the computational experiment. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "    Prepare the theoretical part and computational experiment. Explain the figures, write conclusions. Ready to the second checkpoint. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "    finalisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Code part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-27T19:55:59.693275Z",
     "start_time": "2021-05-27T19:55:59.597099Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'code_p300'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-7a4d6f3682aa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#import patch_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mcode_p300\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mP300Dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mListDataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcode_p300\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdata_dir\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcode_p300\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mvisualizers\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mvis\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'code_p300'"
     ]
    }
   ],
   "source": [
    "#import patch_path\n",
    "from code_p300.datasets import P300Dataset, ListDataset\n",
    "from code_p300.utils import data_dir\n",
    "from code_p300 import visualizers as vis\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-27T19:55:59.696478Z",
     "start_time": "2021-05-27T19:55:59.606Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# dataset collected at our lab\n",
    "# it's the lightest one I know, so I'll use it\n",
    "# in the upcoming weeks it will be public and available through moabb package\n",
    "# also I will test everythong on public datasets like EPFL and BrainInvaders (through our interfaces)\n",
    "# there's an option to work with moabb datasets directly. \n",
    "# But in my opinion, our version is more flexible ad familiar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-27T19:55:59.697620Z",
     "start_time": "2021-05-27T19:55:59.614Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Some records have different number of acts. \n",
    "# It doesn't affect anything, but raises warnings while creating dataset\n",
    "# Will be fixed soon\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category = UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-27T19:55:59.699669Z",
     "start_time": "2021-05-27T19:55:59.619Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "ds_train = P300Dataset(data_dir / 'huge_demons', split = 'train')\n",
    "ds_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-27T19:55:59.701640Z",
     "start_time": "2021-05-27T19:55:59.631Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "ds_val = P300Dataset(data_dir / 'huge_demons', split = 'val')\n",
    "ds_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-27T19:55:59.703530Z",
     "start_time": "2021-05-27T19:55:59.638Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "binds_train = ds_train.binary_dataset()\n",
    "binds_val = ds_val.binary_dataset()\n",
    "binds_train, binds_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-27T19:55:59.705482Z",
     "start_time": "2021-05-27T19:55:59.646Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def binary_transform(bin_ds):\n",
    "        '''Transforms data of binary dataset to friendly format, leaving only target epochs\n",
    "\n",
    "        Args:\n",
    "            bin_ds: BinaryDataset instance\n",
    "\n",
    "        Returns:\n",
    "            list of (epoch, label) tuple\n",
    "        '''\n",
    "\n",
    "        items = []\n",
    "        for epochs_i, labels_i in bin_ds:\n",
    "            for epoch, label in zip(epochs_i, labels_i):\n",
    "                #if label==1:\n",
    "                items.append((torch.flatten(epoch),label))\n",
    "        return items\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-27T19:55:59.707273Z",
     "start_time": "2021-05-27T19:55:59.657Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "data = binary_transform(binds_train)\n",
    "train_set = ListDataset(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-27T19:55:59.712268Z",
     "start_time": "2021-05-27T19:55:59.669Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "data = binary_transform(binds_val)\n",
    "valid_set = ListDataset(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Simple use of AE as encoding (just to remember what it's like)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-27T19:55:59.713836Z",
     "start_time": "2021-05-27T19:55:59.678Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from code_p300.bayesian import  vae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-27T19:55:59.715096Z",
     "start_time": "2021-05-27T19:55:59.686Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import  DataLoader\n",
    "from tqdm import tqdm\n",
    "from torch import optim\n",
    "\n",
    "def train_on_batch(model,\n",
    "                   batch_of_x,\n",
    "                   batch_of_y,\n",
    "                   optimizer):\n",
    "\n",
    "    model.zero_grad()\n",
    "    loss = model.loss(batch_of_x, batch_of_y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return\n",
    "\n",
    "\n",
    "def train_epoch(model,\n",
    "                train_generator,\n",
    "                optimizer):\n",
    "\n",
    "    model.train()\n",
    "    for it, (batch_of_x, batch_of_y) in enumerate(train_generator):\n",
    "        train_on_batch(model, batch_of_x, batch_of_y, optimizer)\n",
    "    return\n",
    "\n",
    "\n",
    "def trainer(model,\n",
    "            optimizer,\n",
    "            dataset,\n",
    "            count_of_epoch=5,\n",
    "            batch_size=64,\n",
    "            progress=None):\n",
    "\n",
    "    iterations = range(count_of_epoch)\n",
    "    \n",
    "    if progress is not None:\n",
    "        iterations = progress(iterations)\n",
    "\n",
    "    for it in iterations:\n",
    "        batch_generator = DataLoader(\n",
    "            dataset=dataset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=True,\n",
    "            pin_memory=True)\n",
    "        train_epoch(\n",
    "            model=model,\n",
    "            train_generator=batch_generator,\n",
    "            optimizer=optimizer)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-27T19:56:00.599061Z",
     "start_time": "2021-05-27T19:55:59.817801Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-27T19:56:00.631007Z",
     "start_time": "2021-05-27T19:56:00.601451Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'code_p300'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-680b5e9df9ff>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mcode_p300\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mclfs_full\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbin_scores\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'code_p300'"
     ]
    }
   ],
   "source": [
    "from code_p300.models import clfs_full, bin_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-27T19:56:00.634696Z",
     "start_time": "2021-05-27T19:55:59.707Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "def encode_epochs(model, bin_ds):\n",
    "    '''Gets BinaryDataset and transforms it to encoder latent space images of epochs and labels\n",
    "\n",
    "    Encodes epochs with `encoder` part of passed model\n",
    "\n",
    "    Args:\n",
    "        model: model with encoder part to get latent representation of epochs\n",
    "        bin_ds: BinaryDataset to transform\n",
    "\n",
    "    Returns:\n",
    "        tuple of arrays of epochs and labels\n",
    "    '''\n",
    "    epochs = []\n",
    "    labels = []\n",
    "    for epochs_i, labels_i in bin_ds:\n",
    "        distr = model.q_z(epochs_i.view(-1, 240))\n",
    "        encoded = model.sample_z(distr)\n",
    "        encoded = encoded.detach()\n",
    "        epochs.append(torch.flatten(encoded, 1))\n",
    "        labels.append(labels_i)\n",
    "        \n",
    "    return torch.cat(epochs).numpy(), np.array(labels)\n",
    "\n",
    "\n",
    "def evaluate_ae(model, bin_train, bin_val):\n",
    "    '''Calculates binary metrics on validation dataset\n",
    "\n",
    "    Transforms dataset with encode_epochs and uses LogisticRegression for predictions\n",
    "\n",
    "    Args:\n",
    "        model: already trained autoencoder to use\n",
    "        dataset_train: dataset to train LogisticRegression on\n",
    "        dataset_val: dataset for evaluation\n",
    "\n",
    "    Returns:\n",
    "        dict of scores values: accuracy, f1_score and roc-auc\n",
    "    '''\n",
    "\n",
    "    encoded_train, labels_train = encode_epochs(model, bin_train)\n",
    "    encoded_val, labels_val = encode_epochs(model, bin_val)\n",
    "\n",
    "    lr = LogisticRegression(class_weight='balanced')\n",
    "    lr.fit(encoded_train, labels_train)\n",
    "    labels_pred = lr.predict(encoded_val)\n",
    "\n",
    "    acc = accuracy_score(labels_val, labels_pred)\n",
    "    f1 = f1_score(labels_val, labels_pred)\n",
    "    roc_auc = roc_auc_score(labels_val, labels_pred)\n",
    "    return {'accuracy': acc, 'f1_score': f1, 'roc-auc': roc_auc}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-27T19:56:00.635782Z",
     "start_time": "2021-05-27T19:55:59.719Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "from sklearn.base import clone\n",
    "\n",
    "from code_p300.types import Directory\n",
    "from code_p300.utils import score, tmp_dir\n",
    "\n",
    "\n",
    "def evaluate_game(\n",
    "    dataset: P300Dataset,\n",
    "    train_acts: int,\n",
    "    classifier,\n",
    "    scores: tuple,\n",
    "    *,\n",
    "    augment_vae= None,\n",
    "    vae = None,\n",
    "    artifacts_dir: Directory = tmp_dir,\n",
    "    verbose: bool = False,\n",
    "    log: bool = True,\n",
    "):\n",
    "    '''Tests given dataset for metrics with default trainable classifier\n",
    "\n",
    "    Args:\n",
    "        dataset: obvious\n",
    "        train_acts: number of first acts for each person to train on\n",
    "        classifier: classifier object with standard sklearn interface\n",
    "\n",
    "    Returns\n",
    "        Dict with means and stds for each calculated metric\n",
    "    '''\n",
    "    # *** appears to be partly private code and depending on private constructs***\n",
    "    # affects only multiclass labels, so must be ok not to use it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-27T19:56:00.636881Z",
     "start_time": "2021-05-27T19:55:59.726Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "SMALL_SIZE = 14\n",
    "MEDIUM_SIZE = 16\n",
    "BIGGER_SIZE = 20\n",
    "\n",
    "plt.rc('font', size=SMALL_SIZE)\n",
    "plt.rc('axes', titlesize=MEDIUM_SIZE)\n",
    "plt.rc('axes', labelsize=MEDIUM_SIZE)\n",
    "plt.rc('xtick', labelsize=SMALL_SIZE)\n",
    "plt.rc('ytick', labelsize=SMALL_SIZE)\n",
    "plt.rc('legend', fontsize=SMALL_SIZE)\n",
    "plt.rc('figure', titlesize=BIGGER_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-27T19:56:00.638303Z",
     "start_time": "2021-05-27T19:55:59.731Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def plot_orig_reconstr(original_x, reconstructed_x):\n",
    "    plt.figure(figsize = (14, 8))\n",
    "    ticks = np.linspace(0, 30, 9)\n",
    "    labels = np.linspace(ds_train.epoch_start*1000, ds_train.epoch_end*1000, 9, dtype=int)\n",
    "    plt.plot(original_x.mean(axis=0), color = 'b', label = 'original eeg')\n",
    "    plt.plot(reconstructed_x.mean(axis=0), color='g', label = 'reconstructed eeg')\n",
    "    plt.xticks(ticks = ticks, labels = labels)\n",
    "    plt.xlabel('miliseconds')\n",
    "    plt.ylabel('normalized mkv')\n",
    "    plt.grid(True)\n",
    "    plt.legend(loc='best')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-27T19:56:00.639625Z",
     "start_time": "2021-05-27T19:55:59.744Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def binary_transform_target(bin_ds):\n",
    "        '''Transforms data of binary dataset to friendly format, leaving only target or non-target epochs\n",
    "\n",
    "        Args:\n",
    "            bin_ds: BinaryDataset instance\n",
    "\n",
    "        Returns:\n",
    "            list of (epoch, label) tuple\n",
    "        '''\n",
    "\n",
    "        items = []\n",
    "        for epochs_i, labels_i in bin_ds:\n",
    "            for epoch, label in zip(epochs_i, labels_i):\n",
    "                if (label.numpy()==1):\n",
    "                    items.append((torch.flatten(epoch),label))\n",
    "        return items\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-27T19:56:00.642306Z",
     "start_time": "2021-05-27T19:55:59.753Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def plot_orig_sampled(original_x, reconstructed_x):\n",
    "    plt.figure(figsize = (14, 8))\n",
    "    ticks = np.linspace(0, 30, 9)\n",
    "    labels = np.linspace(ds_train.epoch_start*1000, ds_train.epoch_end*1000, 9, dtype=int)\n",
    "    plt.plot(original_x.mean(axis=0), color = 'b', label = 'original eeg')\n",
    "    plt.plot(reconstructed_x.mean(axis=0), color='g', label = 'sampled eeg')\n",
    "    plt.xticks(ticks = ticks, labels = labels)\n",
    "    plt.xlabel('miliseconds')\n",
    "    plt.ylabel('normalized mkv')\n",
    "    plt.grid(True)\n",
    "    plt.legend(loc='best')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-27T19:56:00.644838Z",
     "start_time": "2021-05-27T19:55:59.757Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "epoch_shape= binds_train[0][0].shape[1:]\n",
    "epoch_shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### train model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-27T19:56:00.646804Z",
     "start_time": "2021-05-27T19:55:59.766Z"
    },
    "colab": {},
    "colab_type": "code",
    "hidden": true,
    "id": "T4BADLD9RSyn"
   },
   "outputs": [],
   "source": [
    "model = vae.VAE(32, epoch_shape[0]*epoch_shape[1], device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-27T19:56:00.648390Z",
     "start_time": "2021-05-27T19:55:59.771Z"
    },
    "colab": {},
    "colab_type": "code",
    "hidden": true,
    "id": "YrgBN-zJRSyp"
   },
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-27T19:56:00.650108Z",
     "start_time": "2021-05-27T19:55:59.777Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 731
    },
    "colab_type": "code",
    "hidden": true,
    "id": "OM_l95GnRSyq",
    "outputId": "654a8b52-036d-4d96-b921-22d8a15348eb"
   },
   "outputs": [],
   "source": [
    "trainer(model=model,\n",
    "        optimizer=optimizer,\n",
    "        dataset=train_set,\n",
    "        count_of_epoch=30,\n",
    "        batch_size=1024,\n",
    "        progress=tqdm)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-24T12:11:24.223027Z",
     "start_time": "2021-03-24T12:11:24.216197Z"
    },
    "colab": {},
    "colab_type": "code",
    "hidden": true,
    "id": "4xP_DbtohAKf"
   },
   "source": [
    "torch.save(model.state_dict(), './saved_models/vae_d_2.model')\n",
    "#files.download('./models/vae_d_2.model')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-25T17:41:11.129817Z",
     "start_time": "2021-03-25T17:41:11.113644Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "hidden": true,
    "id": "JHWk31EliunG",
    "outputId": "7ad9afae-e53a-424e-9331-6b50cf55063e"
   },
   "source": [
    "model.load_state_dict(torch.load('./saved_models/vae_d_2.model'))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-27T18:13:38.110088Z",
     "start_time": "2021-03-27T18:13:38.093699Z"
    },
    "hidden": true
   },
   "source": [
    "model.state_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-27T19:56:00.652323Z",
     "start_time": "2021-05-27T19:55:59.873Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "evaluate_ae(model, train_set, valid_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-27T19:56:00.654270Z",
     "start_time": "2021-05-27T19:55:59.885Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "benchmark = evaluate_game(ds_train, 5, clfs_full['LR'][0], bin_scores, vae = model, log=False)\n",
    "acc_mean = np.mean(benchmark['mult']['accuracy'])\n",
    "acc_std = np.std(benchmark['mult']['accuracy'])\n",
    "acc_median = np.median(benchmark['mult']['accuracy'])\n",
    "print('train mult_accuracy:',acc_mean.round(3), acc_std.round(3), acc_median.round(3))\n",
    "\n",
    "benchmark = evaluate_game(ds_val, 5, clfs_full['LR'][0], bin_scores, vae = model, log=False)\n",
    "acc_mean = np.mean(benchmark['mult']['accuracy'])\n",
    "acc_std = np.std(benchmark['mult']['accuracy'])\n",
    "acc_median = np.median(benchmark['mult']['accuracy'])\n",
    "print('val mult_accuracy:',acc_mean.round(3), acc_std.round(3), acc_median.round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-27T19:56:00.656471Z",
     "start_time": "2021-05-27T19:55:59.894Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "qz = model.q_z(torch.stack([valid_set[0][0], valid_set[1][0], valid_set[9][0]]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-27T19:56:00.661870Z",
     "start_time": "2021-05-27T19:55:59.902Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "qzsample = model.sample_z(qz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-27T19:56:00.663772Z",
     "start_time": "2021-05-27T19:55:59.910Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model_x = model.q_x(qzsample)[:, 0, :]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-27T19:56:00.665429Z",
     "start_time": "2021-05-27T19:55:59.924Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "original_x = valid_set[9][0].view((8,30)).cpu().data.numpy()\n",
    "reconstructed_x = model_x[2].view((8,30)).cpu().data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-27T19:56:00.667590Z",
     "start_time": "2021-05-27T19:55:59.934Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plot_orig_reconstr(original_x, reconstructed_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-27T19:56:00.669302Z",
     "start_time": "2021-05-27T19:55:59.943Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "original_x = valid_set[1][0].view((8,30)).cpu().data.numpy()\n",
    "reconstructed_x = model_x[1].view((8,30)).cpu().data.numpy()\n",
    "\n",
    "plot_orig_reconstr(original_x, reconstructed_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Augmentations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Target/non-target data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-27T19:56:00.670984Z",
     "start_time": "2021-05-27T19:55:59.957Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "data = binary_transform_target(binds_train)\n",
    "train_set = ListDataset(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-27T19:56:00.672813Z",
     "start_time": "2021-05-27T19:55:59.982Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "data = binary_transform_target(binds_val)\n",
    "valid_set = ListDataset(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-27T19:56:00.674369Z",
     "start_time": "2021-05-27T19:55:59.989Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "alltargets_train = ListDataset(binary_transform(binds_train))\n",
    "alltargets_val = ListDataset(binary_transform(binds_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-27T19:56:00.676139Z",
     "start_time": "2021-05-27T19:56:00.000Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "benchmark = evaluate_game(ds_train, 5, clfs_full['LR'][0], bin_scores, log=False)\n",
    "acc_mean = np.mean(benchmark['mult']['accuracy'])\n",
    "acc_std = np.std(benchmark['mult']['accuracy'])\n",
    "acc_median = np.median(benchmark['mult']['accuracy'])\n",
    "print('train mult_accuracy:',acc_mean.round(3), acc_std.round(3), acc_median.round(3))\n",
    "roc_auc = np.mean(benchmark['bin']['roc_auc'])\n",
    "f1 = np.mean(benchmark['bin']['f1'])\n",
    "print('train roc_auc:', roc_auc.round(3))\n",
    "print('train f1' , f1.round(3))\n",
    "\n",
    "benchmark = evaluate_game(ds_val, 5, clfs_full['LR'][0], bin_scores,  log=False)\n",
    "acc_mean = np.mean(benchmark['mult']['accuracy'])\n",
    "acc_std = np.std(benchmark['mult']['accuracy'])\n",
    "acc_median = np.median(benchmark['mult']['accuracy'])\n",
    "print('val mult_accuracy:',acc_mean.round(3), acc_std.round(3), acc_median.round(3))\n",
    "roc_auc = np.mean(benchmark['bin']['roc_auc'])\n",
    "f1 = np.mean(benchmark['bin']['f1'])\n",
    "print('val roc_auc:', roc_auc.round(3))\n",
    "print('val f1' , f1.round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-27T19:56:00.677893Z",
     "start_time": "2021-05-27T19:56:00.013Z"
    },
    "colab": {},
    "colab_type": "code",
    "hidden": true,
    "id": "T4BADLD9RSyn"
   },
   "outputs": [],
   "source": [
    "model = vae.VAE(64, epoch_shape[0]*epoch_shape[1], device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-27T19:56:00.679755Z",
     "start_time": "2021-05-27T19:56:00.020Z"
    },
    "colab": {},
    "colab_type": "code",
    "hidden": true,
    "id": "YrgBN-zJRSyp"
   },
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-27T19:56:00.681881Z",
     "start_time": "2021-05-27T19:56:00.029Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 731
    },
    "colab_type": "code",
    "hidden": true,
    "id": "OM_l95GnRSyq",
    "outputId": "654a8b52-036d-4d96-b921-22d8a15348eb"
   },
   "outputs": [],
   "source": [
    "trainer(model=model,\n",
    "        optimizer=optimizer,\n",
    "        dataset=train_set,\n",
    "        count_of_epoch=100,\n",
    "        batch_size=1024,\n",
    "        progress=tqdm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "##### Trained only on targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-27T19:56:00.683872Z",
     "start_time": "2021-05-27T19:56:00.038Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "benchmark = evaluate_game(ds_train, 5, clfs_full['LR'][0], bin_scores, augment_vae = model, log=False)\n",
    "acc_mean = np.mean(benchmark['mult']['accuracy'])\n",
    "acc_std = np.std(benchmark['mult']['accuracy'])\n",
    "acc_median = np.median(benchmark['mult']['accuracy'])\n",
    "print('train mult_accuracy:',acc_mean.round(3), acc_std.round(3), acc_median.round(3))\n",
    "roc_auc = np.mean(benchmark['bin']['roc_auc'])\n",
    "f1 = np.mean(benchmark['bin']['f1'])\n",
    "print('train roc_auc:',roc_auc.round(3))\n",
    "print('train f1' , f1.round(3))\n",
    "\n",
    "benchmark = evaluate_game(ds_val, 5, clfs_full['LR'][0], bin_scores,augment_vae = model,  log=False)\n",
    "acc_mean = np.mean(benchmark['mult']['accuracy'])\n",
    "acc_std = np.std(benchmark['mult']['accuracy'])\n",
    "acc_median = np.median(benchmark['mult']['accuracy'])\n",
    "print('val mult_accuracy:',acc_mean.round(3), acc_std.round(3), acc_median.round(3))\n",
    "roc_auc = np.mean(benchmark['bin']['roc_auc'])\n",
    "f1 = np.mean(benchmark['bin']['f1'])\n",
    "print('val roc_auc:', roc_auc.round(3))\n",
    "print('val f1' , f1.round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-27T19:56:00.686022Z",
     "start_time": "2021-05-27T19:56:00.049Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "sample = model.generate_samples(100).mean(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-27T19:56:00.688109Z",
     "start_time": "2021-05-27T19:56:00.053Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "original_x = torch.stack([valid_set[i][0] for i in range(len(valid_set))]).mean(dim=0)\n",
    "original_x = original_x.view((8,30)).cpu().data.numpy()\n",
    "reconstructed_x = sample.view((8,30)).cpu().data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-27T19:56:00.689878Z",
     "start_time": "2021-05-27T19:56:00.058Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plot_orig_sampled(original_x, reconstructed_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "##### Trained on all dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-27T19:56:00.691569Z",
     "start_time": "2021-05-27T19:56:00.064Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "benchmark = evaluate_game(ds_train, 5, clfs_full['LR'][0], bin_scores, augment_vae = model, log=False)\n",
    "acc_mean = np.mean(benchmark['mult']['accuracy'])\n",
    "acc_std = np.std(benchmark['mult']['accuracy'])\n",
    "acc_median = np.median(benchmark['mult']['accuracy'])\n",
    "print('train mult_accuracy:',acc_mean.round(3), acc_std.round(3), acc_median.round(3))\n",
    "roc_auc = np.mean(benchmark['bin']['roc_auc'])\n",
    "f1 = np.mean(benchmark['bin']['f1'])\n",
    "print('train roc_auc:',roc_auc.round(3))\n",
    "print('train f1' , f1.round(3))\n",
    "\n",
    "benchmark = evaluate_game(ds_val, 5, clfs_full['LR'][0], bin_scores,augment_vae = model,  log=False)\n",
    "acc_mean = np.mean(benchmark['mult']['accuracy'])\n",
    "acc_std = np.std(benchmark['mult']['accuracy'])\n",
    "acc_median = np.median(benchmark['mult']['accuracy'])\n",
    "print('val mult_accuracy:',acc_mean.round(3), acc_std.round(3), acc_median.round(3))\n",
    "roc_auc = np.mean(benchmark['bin']['roc_auc'])\n",
    "f1 = np.mean(benchmark['bin']['f1'])\n",
    "print('val roc_auc:', roc_auc.round(3))\n",
    "print('val f1' , f1.round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-27T19:56:00.692963Z",
     "start_time": "2021-05-27T19:56:00.071Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "sample = model.generate_samples(100).mean(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-27T19:56:00.694017Z",
     "start_time": "2021-05-27T19:56:00.081Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "original_x = torch.stack([valid_set[i][0] for i in range(len(valid_set))]).mean(dim=0)\n",
    "original_x = original_x.view((8,30)).cpu().data.numpy()\n",
    "reconstructed_x = sample.view((8,30)).cpu().data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-27T19:56:00.695206Z",
     "start_time": "2021-05-27T19:56:00.087Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plot_orig_reconstr(original_x, reconstructed_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-27T19:56:00.696496Z",
     "start_time": "2021-05-27T19:56:00.104Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-27T19:56:00.698274Z",
     "start_time": "2021-05-27T19:56:00.111Z"
    }
   },
   "outputs": [],
   "source": [
    "from code.datasets import get_data\n",
    "from code.models import NeuralCDE, ODELSTM, LMUCell\n",
    "import torchcde\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-27T19:56:00.700610Z",
     "start_time": "2021-05-27T19:56:00.119Z"
    }
   },
   "outputs": [],
   "source": [
    "LMUCell(input_size = 3, hidden_size = 5, memory_size = 2, theta = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-27T19:56:00.702195Z",
     "start_time": "2021-05-27T19:56:00.134Z"
    }
   },
   "outputs": [],
   "source": [
    "num_epochs=30\n",
    "train_X, train_y = get_data()\n",
    "\n",
    "######################\n",
    "# input_channels=3 because we have both the horizontal and vertical position of a point in the spiral, and time.\n",
    "# hidden_channels=8 is the number of hidden channels for the evolving z_t, which we get to choose.\n",
    "# output_channels=1 because we're doing binary classification.\n",
    "######################\n",
    "model = NeuralCDE(input_channels=3, hidden_channels=8, output_channels=1)\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "######################\n",
    "# Now we turn our dataset into a continuous path. We do this here via natural cubic spline interpolation.\n",
    "# The resulting `train_coeffs` is a tensor describing the path.\n",
    "# For most problems, it's probably easiest to save this tensor and treat it as the dataset.\n",
    "######################\n",
    "train_coeffs = torchcde.natural_cubic_coeffs(train_X)\n",
    "\n",
    "train_dataset = torch.utils.data.TensorDataset(train_coeffs, train_y)\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=32)\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in train_dataloader:\n",
    "        batch_coeffs, batch_y = batch\n",
    "        pred_y = model(batch_coeffs).squeeze(-1)\n",
    "        loss = torch.nn.functional.binary_cross_entropy_with_logits(pred_y, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "    print('Epoch: {}   Training loss: {}'.format(epoch, loss.item()))\n",
    "\n",
    "test_X, test_y = get_data()\n",
    "test_coeffs = torchcde.natural_cubic_coeffs(test_X)\n",
    "pred_y = model(test_coeffs).squeeze(-1)\n",
    "binary_prediction = (torch.sigmoid(pred_y) > 0.5).to(test_y.dtype)\n",
    "prediction_matches = (binary_prediction == test_y).to(test_y.dtype)\n",
    "proportion_correct = prediction_matches.sum() / test_y.size(0)\n",
    "print('Test Accuracy: {}'.format(proportion_correct))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-27T19:56:00.706829Z",
     "start_time": "2021-05-27T19:56:00.141Z"
    }
   },
   "outputs": [],
   "source": [
    "train_X, train_y = get_data()\n",
    "######################\n",
    "# input_channels=3 because we have both the horizontal and vertical position of a point in the spiral, \n",
    "# and time.\n",
    "# hidden_channels=8 is the number of hidden channels for the evolving z_t, which we get to choose.\n",
    "# output_channels=1 because we're doing binary classification.\n",
    "######################\n",
    "model = ODELSTM(in_features=2, hidden_size=8, out_feature=1, return_sequences=False)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0005)\n",
    "\n",
    "######################\n",
    "# Now we turn our dataset into a continuous path. We do this here via natural cubic spline interpolation.\n",
    "# The resulting `train_coeffs` are some tensors describing the path.\n",
    "# For most problems, it's advisable to save these coeffs and treat them as the dataset, \n",
    "# as this interpolation can take a long time.\n",
    "######################\n",
    "#train_coeffs = controldiffeq.natural_cubic_spline_coeffs(train_t, train_X)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size = 10, gamma=0.5)\n",
    "train_dataset = torch.utils.data.TensorDataset(train_X, train_y)\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=32)\n",
    "for epoch in range(30):\n",
    "    for batch in train_dataloader:\n",
    "        batch_x, batch_y = batch\n",
    "        pred_y = model(batch_x[:, :, 1:], batch_x[:, :, 0]).squeeze(-1)\n",
    "        loss = torch.nn.functional.binary_cross_entropy_with_logits(\n",
    "            pred_y, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "    \n",
    "    scheduler.step()\n",
    "    print('Epoch: {}   Training loss: {}    lr: {}'.format(epoch, loss.item(), optimizer.param_groups[0]['lr']))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Old stuff "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-26T13:24:40.686506Z",
     "start_time": "2021-05-26T13:24:40.665330Z"
    },
    "hidden": true
   },
   "source": [
    "import torchcde\n",
    "import math\n",
    "import torch\n",
    "\n",
    "\n",
    "\n",
    "######################\n",
    "# A CDE model looks like\n",
    "#\n",
    "# z_t = z_0 + \\int_0^t f_\\theta(z_s) dX_s\n",
    "#\n",
    "# Where X is your data and f_\\theta is a neural network. So the first thing we need to do is define such an f_\\theta.\n",
    "# That's what this CDEFunc class does.\n",
    "# Here we've built a small single-hidden-layer neural network, whose hidden layer is of width 128.\n",
    "######################\n",
    "class CDEFunc(torch.nn.Module):\n",
    "    def __init__(self, input_channels, hidden_channels):\n",
    "        ######################\n",
    "        # input_channels is the number of input channels in the data X. (Determined by the data.)\n",
    "        # hidden_channels is the number of channels for z_t. (Determined by you!)\n",
    "        ######################\n",
    "        super(CDEFunc, self).__init__()\n",
    "        self.input_channels = input_channels\n",
    "        self.hidden_channels = hidden_channels\n",
    "\n",
    "        self.linear1 = torch.nn.Linear(hidden_channels, 128)\n",
    "        self.linear2 = torch.nn.Linear(128, input_channels * hidden_channels)\n",
    "\n",
    "    ######################\n",
    "    # For most purposes the t argument can probably be ignored; unless you want your CDE to behave differently at\n",
    "    # different times, which would be unusual. But it's there if you need it!\n",
    "    ######################\n",
    "    def forward(self, t, z):\n",
    "        # z has shape (batch, hidden_channels)\n",
    "        z = self.linear1(z)\n",
    "        z = z.relu()\n",
    "        z = self.linear2(z)\n",
    "        ######################\n",
    "        # Easy-to-forget gotcha: Best results tend to be obtained by adding a final tanh nonlinearity.\n",
    "        ######################\n",
    "        z = z.tanh()\n",
    "        ######################\n",
    "        # Ignoring the batch dimension, the shape of the output tensor must be a matrix,\n",
    "        # because we need it to represent a linear map from R^input_channels to R^hidden_channels.\n",
    "        ######################\n",
    "        z = z.view(z.size(0), self.hidden_channels, self.input_channels)\n",
    "        return z\n",
    "\n",
    "\n",
    "######################\n",
    "# Next, we need to package CDEFunc up into a model that computes the integral.\n",
    "######################\n",
    "class NeuralCDE(torch.nn.Module):\n",
    "    def __init__(self, input_channels, hidden_channels, output_channels, interpolation=\"cubic\"):\n",
    "        super(NeuralCDE, self).__init__()\n",
    "\n",
    "        self.func = CDEFunc(input_channels, hidden_channels)\n",
    "        self.initial = torch.nn.Linear(input_channels, hidden_channels)\n",
    "        self.readout = torch.nn.Linear(hidden_channels, output_channels)\n",
    "        self.interpolation = interpolation\n",
    "\n",
    "    def forward(self, coeffs):\n",
    "        if self.interpolation == 'cubic':\n",
    "            X = torchcde.NaturalCubicSpline(coeffs)\n",
    "        elif self.interpolation == 'linear':\n",
    "            X = torchcde.LinearInterpolation(coeffs)\n",
    "        else:\n",
    "            raise ValueError(\"Only 'linear' and 'cubic' interpolation methods are implemented.\")\n",
    "\n",
    "        ######################\n",
    "        # Easy to forget gotcha: Initial hidden state should be a function of the first observation.\n",
    "        ######################\n",
    "        X0 = X.evaluate(X.interval[0])\n",
    "        z0 = self.initial(X0)\n",
    "\n",
    "        ######################\n",
    "        # Actually solve the CDE.\n",
    "        ######################\n",
    "        z_T = torchcde.cdeint(X=X,\n",
    "                              z0=z0,\n",
    "                              func=self.func,\n",
    "                              t=X.interval)\n",
    "\n",
    "        ######################\n",
    "        # Both the initial value and the terminal value are returned from cdeint; extract just the terminal value,\n",
    "        # and then apply a linear map.\n",
    "        ######################\n",
    "        z_T = z_T[:, 1]\n",
    "        pred_y = self.readout(z_T)\n",
    "        return pred_y\n",
    "\n",
    "\n",
    "######################\n",
    "# Now we need some data.\n",
    "# Here we have a simple example which generates some spirals, some going clockwise, some going anticlockwise.\n",
    "######################\n",
    "def get_data(num_timepoints=100):\n",
    "    t = torch.linspace(0., 4 * math.pi, num_timepoints)\n",
    "\n",
    "    start = torch.rand(128) * 2 * math.pi\n",
    "    x_pos = torch.cos(start.unsqueeze(1) + t.unsqueeze(0)) / (1 + 0.5 * t)\n",
    "    x_pos[:64] *= -1\n",
    "    y_pos = torch.sin(start.unsqueeze(1) + t.unsqueeze(0)) / (1 + 0.5 * t)\n",
    "    x_pos += 0.01 * torch.randn_like(x_pos)\n",
    "    y_pos += 0.01 * torch.randn_like(y_pos)\n",
    "    ######################\n",
    "    # Easy to forget gotcha: time should be included as a channel; Neural CDEs need to be explicitly told the\n",
    "    # rate at which time passes. Here, we have a regularly sampled dataset, so appending time is pretty simple.\n",
    "    ######################\n",
    "    X = torch.stack([t.unsqueeze(0).repeat(128, 1), x_pos, y_pos], dim=2)\n",
    "    y = torch.zeros(128)\n",
    "    y[:64] = 1\n",
    "\n",
    "    perm = torch.randperm(128)\n",
    "    X = X[perm]\n",
    "    y = y[perm]\n",
    "\n",
    "    ######################\n",
    "    # X is a tensor of observations, of shape (batch=128, sequence=100, channels=3)\n",
    "    # y is a tensor of labels, of shape (batch=128,), either 0 or 1 corresponding to anticlockwise or clockwise respectively.\n",
    "    ######################\n",
    "    return X, y\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-26T13:33:46.003754Z",
     "start_time": "2021-05-26T13:33:44.214537Z"
    },
    "hidden": true
   },
   "source": [
    "# Copyright 2021 The ODE-LSTM Authors. All Rights Reserved.\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchdyn.models import NeuralDE\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.metrics.functional import accuracy\n",
    "\n",
    "\n",
    "class ODELSTMCell(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, solver_type=\"dopri5\"):\n",
    "        super(ODELSTMCell, self).__init__()\n",
    "        self.solver_type = solver_type\n",
    "        self.fixed_step_solver = solver_type.startswith(\"fixed_\")\n",
    "        self.lstm = nn.LSTMCell(input_size, hidden_size)\n",
    "        # 1 hidden layer NODE\n",
    "        self.f_node = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "        )\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        if not self.fixed_step_solver:\n",
    "            self.node = NeuralDE(self.f_node, solver=solver_type)\n",
    "        else:\n",
    "            options = {\n",
    "                \"fixed_euler\": self.euler,\n",
    "                \"fixed_heun\": self.heun,\n",
    "                \"fixed_rk4\": self.rk4,\n",
    "            }\n",
    "            if not solver_type in options.keys():\n",
    "                raise ValueError(\"Unknown solver type '{:}'\".format(solver_type))\n",
    "            self.node = options[self.solver_type]\n",
    "\n",
    "    def forward(self, input, hx, ts):\n",
    "        new_h, new_c = self.lstm(input, hx)\n",
    "        if self.fixed_step_solver:\n",
    "            new_h = self.solve_fixed(new_h, ts)\n",
    "        else:\n",
    "            indices = torch.argsort(ts)\n",
    "            batch_size = ts.size(0)\n",
    "            device = input.device\n",
    "            s_sort = ts[indices]\n",
    "            s_sort = s_sort + torch.linspace(0, 1e-4, batch_size, device=device)\n",
    "            # HACK: Make sure no two points are equal\n",
    "            trajectory = self.node.trajectory(new_h, s_sort)\n",
    "            new_h = trajectory[indices, torch.arange(batch_size, device=device)]\n",
    "\n",
    "        return (new_h, new_c)\n",
    "\n",
    "    def solve_fixed(self, x, ts):\n",
    "        ts = ts.view(-1, 1)\n",
    "        for i in range(3):  # 3 unfolds\n",
    "            x = self.node(x, ts * (1.0 / 3))\n",
    "        return x\n",
    "\n",
    "    def euler(self, y, delta_t):\n",
    "        dy = self.f_node(y)\n",
    "        return y + delta_t * dy\n",
    "\n",
    "    def heun(self, y, delta_t):\n",
    "        k1 = self.f_node(y)\n",
    "        k2 = self.f_node(y + delta_t * k1)\n",
    "        return y + delta_t * 0.5 * (k1 + k2)\n",
    "\n",
    "    def rk4(self, y, delta_t):\n",
    "        k1 = self.f_node(y)\n",
    "        k2 = self.f_node(y + k1 * delta_t * 0.5)\n",
    "        k3 = self.f_node(y + k2 * delta_t * 0.5)\n",
    "        k4 = self.f_node(y + k3 * delta_t)\n",
    "\n",
    "        return y + delta_t * (k1 + 2 * k2 + 2 * k3 + k4) / 6.0\n",
    "\n",
    "\n",
    "class ODELSTM(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_features,\n",
    "        hidden_size,\n",
    "        out_feature,\n",
    "        return_sequences=True,\n",
    "        solver_type=\"dopri5\",\n",
    "    ):\n",
    "        super(ODELSTM, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.hidden_size = hidden_size\n",
    "        self.out_feature = out_feature\n",
    "        self.return_sequences = return_sequences\n",
    "\n",
    "        self.rnn_cell = ODELSTMCell(in_features, hidden_size, solver_type=solver_type)\n",
    "        self.fc = nn.Linear(self.hidden_size, self.out_feature)\n",
    "\n",
    "    def forward(self, x, timespans, mask=None):\n",
    "        device = x.device\n",
    "        batch_size = x.size(0)\n",
    "        seq_len = x.size(1)\n",
    "        hidden_state = [\n",
    "            torch.zeros((batch_size, self.hidden_size), device=device),\n",
    "            torch.zeros((batch_size, self.hidden_size), device=device),\n",
    "        ]\n",
    "        outputs = []\n",
    "        last_output = torch.zeros((batch_size, self.out_feature), device=device)\n",
    "        for t in range(seq_len):\n",
    "            inputs = x[:, t]\n",
    "            ts = timespans[:, t].squeeze()\n",
    "            hidden_state = self.rnn_cell.forward(inputs, hidden_state, ts)\n",
    "            current_output = self.fc(hidden_state[0])\n",
    "            outputs.append(current_output)\n",
    "            if mask is not None:\n",
    "                cur_mask = mask[:, t].view(batch_size, 1)\n",
    "                last_output = cur_mask * current_output + (1.0 - cur_mask) * last_output\n",
    "            else:\n",
    "                last_output = current_output\n",
    "        if self.return_sequences:\n",
    "            outputs = torch.stack(outputs, dim=1)  # return entire sequence\n",
    "        else:\n",
    "            outputs = last_output  # only last item\n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "neiry",
   "language": "python",
   "name": "neiry"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "352px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
